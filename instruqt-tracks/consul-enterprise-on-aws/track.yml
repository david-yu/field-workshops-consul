slug: consul-enterprise-on-aws
id: stkflr7xh9ud
type: track
title: Consul on AWS
teaser: Consul as a Shared Service on AWS with Consul Enterprise
description: In this track you will set up a Consul shared service on AWS, and connect
  applications across multiple platforms.
icon: https://storage.googleapis.com/instruqt-frontend/assets/hashicorp/tracks/consul.png
tags: []
owner: hashicorp
developers:
- cpu@hashicorp.com
- lance@hashicorp.com
private: true
published: true
challenges:
- slug: provision-vpcs
  id: ujbryneafql8
  type: challenge
  title: 'Infrastructure: Provision VPCs'
  teaser: Create VPCs for Consul shared service and Development teams
  assignment: |-
    You can think of the Cloud CLI terminal as your laptop machine preloaded with cloud access credentials for your company. <br>

    Any any time you can use the AWS console to view your environment.
    AWS CLI commands will be provided for you to interact with AWS throughout this lab. <br>

    Your first task is setting up VPCs for each team.

    Inspect the Terraform code, and then provision the VPCs.

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    You'll notice you have four separate VPCs.

    ```
    aws ec2 describe-vpcs
    ```

    Their CIDR blocks are listed below:

    ```
    terraform-vpc-shared-svcs: 10.1.0.0/16
    terraform-vpc-frontend: 10.2.0.0/16
    terraform-vpc-backend: 10.3.0.0/16
    ```

    The shared service VPC will run the core Consul infrastructure.
    You will provision the Consul servers into this VPC in the next few assignments.
  notes:
  - type: text
    contents: |-
      In this assignment you will familiarize yourself with the AWS cloud
      environment, and provision VPCs for your services and applications.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/01-provision-vpcs.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vpc
  difficulty: basic
  timelimit: 1200
- slug: build-consul-image
  id: x3xlgqyvidby
  type: challenge
  title: 'Operations: Build Consul Image'
  teaser: Create a Consul immutable image
  assignment: |-
    The Packer build has been set up for you, which you can optionally look through in the code editor.
    This Packer build is provided by the HashiCorp implementation services team. <br>

    Inspect the variable file, then build the EC2 image.

    ```
    cat /root/packer/vars.json
    AWS_REGION=us-east-1 packer build -var-file vars.json centos.json
    ```

    Validate your AMI is available.

    ```
    aws ec2 describe-images --owners self
    ```

    Now that you have a Consul image, you're ready to provision the Consul servers in an ASG.
  notes:
  - type: text
    contents: |-
      In this assignment you will build an immutable image of Consul with [HashiCorp Packer](https://packer.io/). <br>

      Immutability has many advantages for infrastructure management.
      Consul Enterprise can take advantage of immutable patterns with [Automated Upgrades](https://www.consul.io/docs/enterprise/upgrades/index.html).
  tabs:
  - title: Packer
    type: code
    hostname: cloud-client
    path: /root/packer
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/02-build-consul-image.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: provision-consul
  id: jhvf5b2rlhmk
  type: challenge
  title: 'Operations: Provision Consul'
  teaser: Provision Consul severs in an ASG
  assignment: |-
    A terraform module has been provided for you to provision your Consul immutable image into the ASG.
    This module is provided by the HashiCorp implementation services team. <br>

    Inspect the variable file for Terraform, and deploy Consul.

    ```
    cat terraform.tfvars
    cat main.tf
    terraform plan
    terraform apply -auto-approve
    ```

    Now, validate the Consul servers deployed to your ASG. <br>

    ```
    aws autoscaling describe-auto-scaling-groups
    ```

    You should see the Consul servers with a ``*-0.0.1` postfix.
    You will increment the deployment in the next exercise to finish the immutable bootstrap.

    Let's inspect one of the newly provisioned Consul instances.
    We can access the newly provisioned instance through our bastion host helper script.

    ```
    check-consul-config /etc/consul.d/consul.hcl
    ```

    You will finish setting up the Consul servers in the next assignment.
  notes:
  - type: text
    contents: In this assignment, you'll install Consul into ASG using the immutable
      image you created in the last assignment. <br>
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/03-provision-consul.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: bootstrap-consul
  id: 7mefmboypk7h
  type: challenge
  title: 'Operations: Bootstrap Consul'
  teaser: Bootstrap Consul with an immutable migration
  assignment: |-
    In one of the terminal window, you will watch the new servers transition in real time.

    ```
    watch consul operator raft list-peers
    ```

    You will notice two changes to the Terraform variables to complete the bootstrapping process.
      * consul_cluster_version = "0.0.2"
      * bootstrap = false

    Now run terraform, and watch the servers automatically transition with the watch command you configured earlier.

    ```
    cat terraform.tfvars
    cat main.tf
    terraform plan
    terraform apply -auto-approve
    ```

    You can now validate the `node_meta` is for the `0.0.2` version of the deployment. <br>

    ```
    consul catalog nodes --detailed
    ```

    You can also see that the `default_policy` for ACLs was updated to deny, the desired production value.

    ```
    check-consul-config /etc/consul.d/consul.hcl
    ```

    In the next assignment you will centralize secrets from the Consul bootstrap for service consumers.
  notes:
  - type: text
    contents: |-
      Consul Enterprise supports an [upgrade pattern](https://learn.hashicorp.com/consul/day-2-operations/autopilot#upgrade-migrations) that allows operators to deploy a complete cluster of new servers,
      and safely migrate the new servers until the upgrade is complete. <br>

      This feature allows you to easily apply codified changes to the Consul servers without impacting availability of the service.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/04-bootstrap-consul.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  difficulty: basic
  timelimit: 1200
- slug: centralize-consul-secrets
  id: 8whoxkkzchx0
  type: challenge
  title: 'Security: Centralize Consul Secrets'
  teaser: Use Vault to store Consul secrets
  assignment: |-
    The Consul bootstrap outputs from the last few assignments have been secured in Vault through the provisioning process. <br>

    Login as an operator and inspect the credentials. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    ```

    Now inspect the credentials.

    ```
    vault kv get secret/consul
    ```

    You can use the master token to create a management token for Vault to issue [dynamic secrets](https://www.vaultproject.io/docs/secrets/consul/) for Consul.

    Get a management token for Vault to manage Consul tokens with.
    You can retrieve the privileged token for this operation from Vault.  <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault kv get -field=master_token secret/consul)
    vault_consul_mgmt_token=$(consul acl token create -policy-name=global-management -description "vault mgmt" | grep SecretID | cut -d ":" -f2 | xargs)
    ```

    Now configure the secrets engine.

    ```
    vault write consul/config/access address=${CONSUL_HTTP_ADDR} token=${vault_consul_mgmt_token}
    vault read consul/config/access
    ```

    Last, create a policy for the operations team, and link it to the Vault role.

    ```
    consul acl policy create -name "ops" -description "admin policy for ops" -rules 'acl = "write" operator = "write" namespace_prefix "" {acl = "write"}'
    vault write consul/roles/ops policies=ops ttl=1h
    ```

    Now you are ready to get a dynamic Consul token from Vault for an operator.
    Validate the token after you fetch it. <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    You will use this role in a later assignment to configure access for Consul service consumers.
  notes:
  - type: text
    contents: |-
      Terraform and Vault are commonly used together to provide secure provisioning workflows.
      Terraform Enterprise provides additional features on top of Terraform to make securing this workflow easier. <br>

      In this assignment you will use these highly privileged credentials to configure self-service,
      least privileged access to the Consul service. <br>

      A Vault instance is provisioning for this assignment. Please be patient as this can take a few minutes.
  tabs:
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vault
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/05-centralize-consul-secrets.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: validate-backup-agent
  id: iq5nherggpo9
  type: challenge
  title: 'Operations: Validate Backup Agent'
  teaser: Automatically Backup Consul to S3
  assignment: |-
    Automated Backups requires a fully licensed Consul Enterprise environment to function properly.
    This environment currently has a temporary license. <br>

    If you have an enterprise license (your instructor will provide one for you) continue with the assignment.
    If you do not have an enterprise license, click `check` now and skip this section. <br>

    The Consul snapshot agent is highly available, and coordinates with other snapshot agents by establishing a [session](https://www.consul.io/docs/internals/sessions.html) in Consul.
    You will validate the session, and check S3 for the Consul snapshot backup files. <br>

    Get a privileged token to perform the license action. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Now apply the license. <br>

    ```
    consul license put <license>
    sleep 60
    ```

    Inspect the configure file for the agent that was applied by the Terraform module. <br>

    ```
    check-consul-config /etc/consul-snapshot.d/consul-snapshot.json | jq
    ```

    The Snapshot Agent service should now be healthy.

    ```
    curl -s ${CONSUL_HTTP_ADDR}/v1/health/service/consul-snapshot?passing=true | jq
    ```

    Now you can retrieve the session that is holding the lock. <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault kv get -field snapshot_token secret/consul)
    consul acl token read -self
    consul kv get -detailed consul-snapshot/lock
    ```

    The session info holds the address of the active backup agent. <br>

    ```
    session=$(consul kv get -detailed consul-snapshot/lock | grep Session | awk -F ' ' '{print $2}')
    curl -v --header "X-Consul-Token: ${CONSUL_HTTP_TOKEN}" $CONSUL_HTTP_ADDR/v1/session/info/${session} | jq
    ```

    With a session lock present, you should have Consul backups in the S3 bucket.

    ```
    aws s3 ls
    bucket=$(aws s3api list-buckets --query "Buckets[].Name" | jq -r .[0])
    aws s3 ls s3://${bucket}/consul-snapshot/
    ```

    Consul is now automatically backing up to S3. In the next assignment you will start configuring the network.
  notes:
  - type: text
    contents: |-
      Consul Enterprise enables you to run the snapshot agent within your environment.
      Once running, the snapshot agent service operates as a highly available process that integrates with the snapshot API to automatically manage taking snapshots, backup rotation, and sending backup files offsite to Amazon S3.

      In this assignment you will validate the Snapshot Agent is working correctly.
  tabs:
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/vault
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/05-centralize-consul-secrets.html
  difficulty: basic
  timelimit: 1200
- slug: platform-quiz
  id: yreyomgnhyt9
  type: quiz
  title: 'Quiz: Platform'
  teaser: Platform Review
  assignment: |
    What feature(s) did you use to set up your Consul shared service in the lab?
  notes:
  - type: text
    contents: |
      Quiz time!
  answers:
  - Automated Upgrades
  - Automated Backups
  - Consul Sessions
  - None of the Above
  solution:
  - 0
  - 1
  - 2
  difficulty: basic
  timelimit: 300
- slug: create-vpc-peering
  id: suzx6ufcymhe
  type: challenge
  title: 'Network: Create Transitive Peering'
  teaser: Add connectivity to the Consul shared service
  assignment: |-
    Run Terraform to setup the transit gateway routes between VPCs you provisioned earlier. <br>

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    Inspect the route table for the backend team private subnets.

    ```
    route_table=$(/usr/local/bin/terraform output -state /root/terraform/vpc/terraform.tfstate -json backend_private_route_table_ids | jq -r .[0])
    aws ec2 describe-route-tables --route-table-ids ${route_table}
    ```

    You'll notice routes to CIDR blocks for shared services route target the TGW. <br>

    In a later assignment you will use these TGW routes to connect the mesh across VPCs.
  notes:
  - type: text
    contents: |-
      [AWS Transit Gateway](https://aws.amazon.com/transit-gateway/) can help reduce complexity of VPC peering in AWS
      environments, and is commonly used to help developers connect their applications to shared services. <br>

      In this assignment you will use TGW to provide connectivity across VPCs.
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/08-create-vpc-peering.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/tgw
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1200
- slug: provision-eks-clusters
  id: oxwep1bahgcl
  type: challenge
  title: 'Infrastructure: Provision EKS clusters'
  teaser: Deploy EKS clusters for development teams
  assignment: |-
    Inspect the Terraform for EKS. <br>

    EKS can take over ~15 minutes to provision, so this task we kicked off in the background during your other assignments.
    You can see the log output below. <br>

    ```
    tail -f terraform.out
    ```

    Validate your clusters once they are provisioned.
    Their status will be `ACTIVE`. <br>

    ```
    aws eks describe-cluster --name frontend
    aws eks describe-cluster --name backend
    ```

    In the next assignment you will configure connectivity for these clusters back to the shared Consul service.
  notes:
  - type: text
    contents: |-
      Development teams are provided with EKS clusters in their VPCs. <br>

      In this assignment you will provision EKS clusters for application workloads.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/09-provision-eks-clusters.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/eks
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  difficulty: basic
  timelimit: 1200
- slug: configure-eks-cluster-segments
  id: xrckckfdqgj9
  type: challenge
  title: 'Operations: Configure EKS cluster segments'
  teaser: Add Consul network segments for hub and spoke connectivity
  assignment: |-
    The network segment config has been added to the Terraform file with a new cluster version for Consul.
    Inspect it, then apply the configuration. <br>

    ```
    cat terraform.tfvars
    cat main.tf
    terraform plan
    terraform apply -auto-approve
    ```

    Validate your network segments were applied to the Consul servers.

    ```
    check-consul-config /etc/consul.d/zz-override.json | jq
    ```

    Now that the Consul service is stood up, and connectivity is in place, Consul operators can configure the tenants for self-service.
  notes:
  - type: text
    contents: |-
      Network policies for cross-vpc traffic are restricted to proxy authenticated traffic. <br>

      Consul enterprise [network segments](https://www.consul.io/docs/enterprise/network-segments/index.html) support this hub-and-spoke topology.
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/10-configure-eks-cluster-segments.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Terraform
    type: code
    hostname: cloud-client
    path: /root/terraform/consul
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1200
- slug: gvrs-quiz
  id: kjjm5qjzltq0
  type: quiz
  title: 'Quiz: GVRS'
  teaser: Visibility, Routing, and Scale Review
  assignment: |
    What feature(s) did you use to create Consul gossip segments across AWS Transit Gateway in the lab?
  notes:
  - type: text
    contents: |
      Quiz time!
  answers:
  - Advanced Federation
  - Network Segmentation
  - Redundancy Zones
  - None of the Above
  solution:
  - 1
  difficulty: basic
  timelimit: 300
- slug: create-namespaces-and-policies
  id: g7hylnrby0x9
  type: challenge
  title: 'Operations: Create Namespaces and Policies'
  teaser: Configure self-service access
  assignment: |-
    You need a privileged operator token to configure Consul for the consuming teams.
    Authenticate with Vault, and fetch a token. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    The service discovery policy will underpin all development tenants.
    This will allow the mesh to discover services across teams, however, individuals teams will control what services can connect to them from other namespaces. <br>

    ```
    consul acl policy create -name "cross-namespace-policy" -description "cross-namespace service discovery" -rules @cross-namespace.hcl
    ```

    The K8s injector policy will allow read access to the namespaces available within the Consul service.
    This is required for the Consul K8s integration to work properly. <br>

    ```
    consul acl policy create -name "k8s-injector-policy" -description "k8s injection" -rules @injector.hcl
    ```

    Now you can create the namespaces for each development team, that share the namespace discovery policy. <br>

    ```
    consul namespace write frontend-namespace.hcl
    consul namespace write backend-namespace.hcl
    ```

    You also need to create policies for low level client agent permissions for each development VPC.
    We will create a blast radius for these tokens by CIDR block. <br>

    ```
    consul acl policy create -name "frontend-agent-policy" -description "frontend agents" -rules @frontend-team-agent.hcl
    consul acl policy create -name "backend-agent-policy" -description "backend agents" -rules @backend-team-agent.hcl
    ```

    Last, create policies for the development teams to manage their own intentions.
    Intentions allow connectivity between services. <br>

    ```
    consul acl policy create -name "frontend-developer-policy" -description "frontend dev" -rules @frontend-team-developer.hcl
    consul acl policy create -name "backend-developer-policy" -description "backend dev" -rules @backend-team-developer.hcl

    ```

    In the next assignment we will link these policies to dynamic Vault roles so developers and operators can get access centrally.
  notes:
  - type: text
    contents: Consul enterprise support [namespaces](https://www.consul.io/docs/enterprise/namespaces/index.html)
      to enable network and operations teams to provide self-service for development
      teams.
  tabs:
  - title: Policies
    type: code
    hostname: cloud-client
    path: /root/policies
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/06-create-namespaces-and-policies.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: create-consul-roles
  id: r0oxqx8ait5u
  type: challenge
  title: 'Security: Create Consul Roles'
  teaser: Create Dynamic Consul roles in Vault
  assignment: |-
    Authenticate with the Consul operations role.

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    Next, link the policies. The TTLs are scoped for each task.

    ```
    vault write consul/roles/k8s-injector           policies=k8s-injector-policy          ttl=720h
    vault write consul/roles/frontend-agent         policies=frontend-agent-policy        ttl=720h
    vault write consul/roles/frontend-developer     policies=frontend-developer-policy    ttl=1h
    vault write consul/roles/backend-agent          policies=backend-agent-policy         ttl=720h
    vault write consul/roles/backend-developer      policies=backend-developer-policy     ttl=1h
    ```

    You will use these roles in later assignments to seed K8s clusters with scoped access to the central service,
    as well as allow developers the ability to configure routing within their tenant.
  notes:
  - type: text
    contents: |-
      Vault is the leading solution for secrets management across multi-cloud and hybrid deployments. <br>
      In this assignment you will enable self-service access to Consul credentials for developers and operators.
  tabs:
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/07-create-consul-roles.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: configure-eks-cluster-secrets
  id: ld9gbmuj5jnr
  type: challenge
  title: 'Operations: Configure EKS cluster secrets'
  teaser: Create K8s secrets for Consul
  assignment: |-
    You need to authenticate with Vault to gain access to the Consul roles required to provide secrets for the K8s clusters.

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    A sample script has been provided to setup each K8s cluster.
    The script will perform the following tasks:

    * Create a namespace to run Consul pods (injector webhooks, daemonset, etc.)
    * Provide a Consul gossip key
    * Provide a Consul ACL token

    Inspect the setup script.

    ```
    cat /usr/local/bin/setup-k8s-consul-cluster
    ```

    Start with the frontend cluster.

    ```
    kubectl config use-context eks_frontend
    setup-k8s-consul-cluster $(vault kv get -field gossip_key secret/consul) $(vault read -field token consul/creds/frontend-agent) $(vault read -field token consul/creds/k8s-injector)
    kubectl  get secrets -n consul
    ```

    Repeat, for the backend cluster.

    ```
    kubectl config use-context eks_backend
    setup-k8s-consul-cluster $(vault kv get -field gossip_key secret/consul) $(vault read -field token consul/creds/backend-agent) $(vault read -field token consul/creds/k8s-injector)
    kubectl get secrets -n consul
    ```

    In the next assignment you will use these secrets to deploy the Consul client agents with Helm.
  notes:
  - type: text
    contents: |-
      [Consul Helm](https://www.consul.io/docs/platform/k8s/helm.html) is the HashiCorp supported method for deploying Consul into K8s environment,
      and it supports a wide array of configuration options. <br>

      In this assignment you will configure K8s secrets that will allow client agents running in K8s
      to authenticate with Consul servers in the shared service.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/11-configure-eks-cluster-secrets.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: deploy-consul-in-eks
  id: 3ewjpnxsjsmh
  type: challenge
  title: 'Operations: Deploy Consul in EKS'
  teaser: Deploy Consul in EKS with Helm
  assignment: |-
    Use the code editor to inspect the Helm chart values for each K8s cluster.
    When you have reviewed the values you can deploy each K8s cluster.

    Start with the frontend cluster.

    ```
    kubectl config use-context eks_frontend
    helm install hashicorp -f frontend-values.yaml --namespace consul ./consul-helm --wait
    kubectl get pods -n consul
    ```

    Repeat for the backend cluster.

    ```
    kubectl config use-context eks_backend
    helm install hashicorp -f backend-values.yaml --namespace consul ./consul-helm --wait
    kubectl get pods -n consul
    ```

    Check the nodes for the full working set.
    You can continue when the Frontend and Backend EKS nodes have joined the peer set. <br>

    ```
    watch consul members
    ```

    In the next assignment we will create trust between the K8s API server and the Consul shared service.
  notes:
  - type: text
    contents: |-
      The k8s clusters are now ready to connect to the Consul shared service.
      You will deploy the Consul client agents with Helm in this assignment.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/12-deploy-consul-in-eks.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Helm
    type: code
    hostname: cloud-client
    path: /root/helm
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  difficulty: basic
  timelimit: 1200
- slug: configure-k8s-consul-authentication
  id: tnkxgigifkbu
  type: challenge
  title: 'Security: Configure K8s Consul Authentication'
  teaser: Create trust between K8s clusters and Consul
  assignment: |-
    Configuring Consul auth methods is a privileged action.
    You need a Consul operator token to perform these tasks. <br>

    ```
    vault login -method=userpass username=operations password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    The cross namespace policy you created earlier will be attached to the token.
    You need three key pieces of information to complete this task. <br>

    * K8s endpoint
    * K8s CA
    * Service Account JWT

    The helper script will assist you. Review it now. <br>

    ```
    cat /usr/local/bin/setup-k8s-consul-auth
    ```

    Run the script for each K8s cluster.

    ```
    setup-k8s-consul-auth frontend
    setup-k8s-consul-auth backend
    ```

    In the next assignment you'll use the trust relationship you just created to deploy applications into the mesh.
  notes:
  - type: text
    contents: |-
      In this assignment you will use the [K8s Consul auth method](https://www.consul.io/docs/acl/auth-methods/kubernetes.html)
      to securely introduce Consul ACL tokens to K8s pods that run applications. <br>

      [Consul auth methods](https://www.consul.io/docs/acl/acl-auth-methods.html) make it easy to securely introduce Consul ACL tokens on ephemeral platforms.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/13-configure-k8s-consul-authentication.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: deploy-applications
  id: dslh2b4hvc4t
  type: challenge
  title: 'Developer: Deploy Applications'
  teaser: Deploy application workloads across K8s environments
  assignment: |-
    You can view the K8s spec files for the applications in the code editor. <br>

    Start with the Frontend cluster application. <br>

    ```
    kubectl config use-context eks_frontend
    kubectl apply -f frontend
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=web -o name) --timeout=30s
    ```

    Now deploy the backend cluster applications. <br>

    ```
    kubectl config use-context eks_backend
    kubectl apply -f backend
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=api -o name) --timeout=30s
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=cache -o name) --timeout=30s
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=currency -o name) --timeout=30s
    kubectl wait --for=condition=Ready $(kubectl get pod --selector=app=payments -o name) --timeout=30s
    ```

    In the next assignment we will update intentions so the new applications will receive traffic over mTLS.
  notes:
  - type: text
    contents: The K8s clusters are now ready to run application workloads.
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/14-deploy-applications.html
  - title: Apps
    type: code
    hostname: cloud-client
    path: /root/apps
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: update-intentions
  id: apxx5zldywqm
  type: challenge
  title: 'Developer: Update Intentions'
  teaser: Routing as a service
  assignment: |-
    Retrieve a Consul token as backend developer team. <br>

    ```
    vault login -method=userpass username=backend password=Password1
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/backend-developer)
    consul acl token read -self
    ```

    Create the intentions for services within your namespace.
    Also allow access from the Frontend Web application to the Backend API application. <br>

    ```
    consul intention create --allow frontend/web backend/api
    consul intention create --allow backend/api backend/cache
    consul intention create --allow backend/api backend/payments
    consul intention create --allow backend/payments backend/currency
    ```
    
    Inside of the Consul UI, click on ACLs and insert the value of the CONSUL_HTTP_TOKEN to view the intentions you created inside of the UI. 

    In the next assignment you will send traffic to the Web application in the Frontend namespace.
  notes:
  - type: text
    contents: |-
      Intentions allow service mesh routing based on service identity. <br>
      The Consul shared service allows developers to control routing within their namespaces, and selectively allow traffic from other teams to their namespace.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/15-update-intentions.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: test-application
  id: jllgtf6nz3hz
  type: challenge
  title: 'Developer: Test Application'
  teaser: Put it all together
  assignment: |-
    You need to fetch the external DNS name for the AWS ELB.

    ```
    kubectl config use-context eks_frontend
    endpoint=$(kubectl get services web -o json | jq -r .status.loadBalancer.ingress[0].hostname)
    ```

    Test the application. You should receive a `200` status code.
    Take note of the IP addresses of each service.

    ```
    curl -v -s $endpoint
    ```

    Well done! You just securely connected application workloads across multiple K8s clusters!!!
  notes:
  - type: text
    contents: |-
      The Web application in the Frontend namespace is exposed via an AWS ELB.
      It can take a few minutes for an ELB to resolve DNS. Please be patient.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: cloud-client
    path: /
    port: 80
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Consul
    type: service
    hostname: cloud-client
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: cloud-client
    path: /
    port: 8200
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/15-update-intentions.html
  - title: Target lab architecture
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/consul-enterprise-on-aws/assets/diagrams/16-final-architecture.html
  difficulty: basic
  timelimit: 1200
- slug: governance-and-policy-quiz
  id: i8yzbimnbcd7
  type: quiz
  title: 'Quiz: Governance & Policy'
  teaser: Governance & Policy Review
  assignment: |
    What feature(s) did you use to enable self-service for developers?
  notes:
  - type: text
    contents: |
      Quiz time!
  answers:
  - ACLs
  - Namespaces
  - Auth Methods
  - Binding Rules
  solution:
  - 1
  difficulty: basic
  timelimit: 300
checksum: "17453291565983350328"
